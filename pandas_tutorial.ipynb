{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Tutorial: Complete Guide for Data Analysis\n",
    "\n",
    "This notebook provides a comprehensive introduction to pandas, Python's most popular data manipulation and analysis library.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Pandas](#introduction)\n",
    "2. [Data Structures: Series and DataFrame](#data-structures)\n",
    "3. [Reading and Writing Data](#io-operations)\n",
    "4. [Data Inspection and Exploration](#data-inspection)\n",
    "5. [Data Selection and Indexing](#selection-indexing)\n",
    "6. [Data Cleaning](#data-cleaning)\n",
    "7. [Data Transformation](#data-transformation)\n",
    "8. [Grouping and Aggregation](#grouping-aggregation)\n",
    "9. [Merging and Joining](#merging-joining)\n",
    "10. [Time Series Analysis](#time-series)\n",
    "11. [Data Visualization with Pandas](#visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Pandas {#introduction}\n",
    "\n",
    "Pandas is a powerful Python library for data manipulation and analysis. It provides:\n",
    "- Fast, flexible data structures (Series and DataFrame)\n",
    "- Tools for reading/writing data from various formats\n",
    "- Data cleaning and preparation capabilities\n",
    "- Statistical analysis functions\n",
    "- Integration with other Python libraries\n",
    "\n",
    "Let's start by importing pandas and other useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.0.2\n",
      "NumPy version: 1.24.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Structures: Series and DataFrame {#data-structures}\n",
    "\n",
    "### Series\n",
    "A Series is a one-dimensional array-like object with labeled indices. Think of it as a column in a spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series from list:\n",
      "0    25\n",
      "1    30\n",
      "2    35\n",
      "3    40\n",
      "4    45\n",
      "Name: Age, dtype: int64\n",
      "Data type: int64\n",
      "Name: Age\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a Series from a list\n",
    "ages = pd.Series([25, 30, 35, 40, 45], name='Age')\n",
    "print(\"Series from list:\")\n",
    "print(ages)\n",
    "print(f\"Data type: {ages.dtype}\")\n",
    "print(f\"Name: {ages.name}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series with custom index:\n",
      "Alice      85\n",
      "Bob        92\n",
      "Charlie    78\n",
      "David      96\n",
      "Eve        88\n",
      "Name: Test_Score, dtype: int64\n",
      "Index: ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a Series with custom index\n",
    "scores = pd.Series([85, 92, 78, 96, 88], \n",
    "                   index=['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "                   name='Test_Score')\n",
    "print(\"Series with custom index:\")\n",
    "print(scores)\n",
    "print(f\"Index: {list(scores.index)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series from dictionary:\n",
      "New York       8419000\n",
      "Los Angeles    3980000\n",
      "Chicago        2716000\n",
      "Houston        2328000\n",
      "Name: Population, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creating a Series from a dictionary\n",
    "city_population = pd.Series({\n",
    "    'New York': 8_419_000,\n",
    "    'Los Angeles': 3_980_000,\n",
    "    'Chicago': 2_716_000,\n",
    "    'Houston': 2_328_000\n",
    "}, name='Population')\n",
    "print(\"Series from dictionary:\")\n",
    "print(city_population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "A DataFrame is a two-dimensional data structure with labeled rows and columns. It's like a spreadsheet or SQL table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame from dictionary:\n",
      "      Name  Age         City  Salary\n",
      "0    Alice   25     New York   70000\n",
      "1      Bob   30  Los Angeles   80000\n",
      "2  Charlie   35      Chicago   90000\n",
      "3    David   40      Houston   95000\n",
      "4      Eve   45      Phoenix   85000\n",
      "\n",
      "Shape: (5, 4)\n",
      "Columns: ['Name', 'Age', 'City', 'Salary']\n",
      "Index: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Creating a DataFrame from a dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 35, 40, 45],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],\n",
    "    'Salary': [70000, 80000, 90000, 95000, 85000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame from dictionary:\")\n",
    "print(df)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Index: {list(df.index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reading and Writing Data {#io-operations}\n",
    "\n",
    "Pandas can read data from various file formats including CSV, Excel, JSON, SQL databases, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data saved to CSV:\n",
      "      Product     Category   Price  Stock  Rating\n",
      "0      Laptop  Electronics  999.99     50     4.5\n",
      "1       Mouse  Accessories   29.99    200     4.2\n",
      "2    Keyboard  Accessories   79.99    150     4.0\n",
      "3     Monitor  Electronics  299.99     75     4.8\n",
      "4  Headphones  Accessories  149.99    100     4.3\n"
     ]
    }
   ],
   "source": [
    "# Create sample data and save to CSV for demonstration\n",
    "sample_data = pd.DataFrame({\n",
    "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones'],\n",
    "    'Category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Accessories'],\n",
    "    'Price': [999.99, 29.99, 79.99, 299.99, 149.99],\n",
    "    'Stock': [50, 200, 150, 75, 100],\n",
    "    'Rating': [4.5, 4.2, 4.0, 4.8, 4.3]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "sample_data.to_csv('sample_products.csv', index=False)\n",
    "print(\"Sample data saved to CSV:\")\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read from CSV:\n",
      "      Product     Category   Price  Stock  Rating\n",
      "0      Laptop  Electronics  999.99     50     4.5\n",
      "1       Mouse  Accessories   29.99    200     4.2\n",
      "2    Keyboard  Accessories   79.99    150     4.0\n",
      "3     Monitor  Electronics  299.99     75     4.8\n",
      "4  Headphones  Accessories  149.99    100     4.3\n",
      "\n",
      "Data types:\n",
      "Product      object\n",
      "Category     object\n",
      "Price       float64\n",
      "Stock         int64\n",
      "Rating      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Reading from CSV\n",
    "df_csv = pd.read_csv('sample_products.csv')\n",
    "print(\"Data read from CSV:\")\n",
    "print(df_csv)\n",
    "print(f\"\\nData types:\\n{df_csv.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom CSV reading:\n",
      "             Price  Rating\n",
      "Product                   \n",
      "Laptop      999.99     4.5\n",
      "Mouse        29.99     4.2\n",
      "Keyboard     79.99     4.0\n",
      "Monitor     299.99     4.8\n",
      "Headphones  149.99     4.3\n",
      "\n",
      "Data types:\n",
      "Price     float64\n",
      "Rating    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Reading with specific parameters\n",
    "df_custom = pd.read_csv('sample_products.csv', \n",
    "                        index_col='Product',  # Set Product as index\n",
    "                        dtype={'Stock': 'int32'},  # Specify data type\n",
    "                        usecols=['Product', 'Price', 'Rating'])  # Select specific columns\n",
    "\n",
    "print(\"Custom CSV reading:\")\n",
    "print(df_custom)\n",
    "print(f\"\\nData types:\\n{df_custom.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Inspection and Exploration {#data-inspection}\n",
    "\n",
    "Before working with data, it's crucial to understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic information about the DataFrame:\n",
      "Shape: (5, 5)\n",
      "Size: 25\n",
      "Columns: ['Product', 'Category', 'Price', 'Stock', 'Rating']\n",
      "Index: [0, 1, 2, 3, 4]\n",
      "\n",
      "First 3 rows:\n",
      "    Product     Category   Price  Stock  Rating\n",
      "0    Laptop  Electronics  999.99     50     4.5\n",
      "1     Mouse  Accessories   29.99    200     4.2\n",
      "2  Keyboard  Accessories   79.99    150     4.0\n",
      "\n",
      "Last 2 rows:\n",
      "      Product     Category   Price  Stock  Rating\n",
      "3     Monitor  Electronics  299.99     75     4.8\n",
      "4  Headphones  Accessories  149.99    100     4.3\n"
     ]
    }
   ],
   "source": [
    "# Use our sample data for exploration\n",
    "df = sample_data.copy()\n",
    "\n",
    "print(\"Basic information about the DataFrame:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Size: {df.size}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Index: {list(df.index)}\")\n",
    "print()\n",
    "\n",
    "# First few rows\n",
    "print(\"First 3 rows:\")\n",
    "print(df.head(3))\n",
    "print()\n",
    "\n",
    "# Last few rows\n",
    "print(\"Last 2 rows:\")\n",
    "print(df.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types and memory info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Product   5 non-null      object \n",
      " 1   Category  5 non-null      object \n",
      " 2   Price     5 non-null      float64\n",
      " 3   Stock     5 non-null      int64  \n",
      " 4   Rating    5 non-null      float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 328.0+ bytes\n",
      "\n",
      "Data types:\n",
      "Product      object\n",
      "Category     object\n",
      "Price       float64\n",
      "Stock         int64\n",
      "Rating      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Data types and memory usage\n",
    "print(\"Data types and memory info:\")\n",
    "df.info()\n",
    "print()\n",
    "\n",
    "# Data types only\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical summary (numeric columns):\n",
      "            Price       Stock    Rating\n",
      "count    5.000000    5.000000  5.000000\n",
      "mean   311.990000  115.000000  4.360000\n",
      "std    397.831623   60.207973  0.304959\n",
      "min     29.990000   50.000000  4.000000\n",
      "25%     79.990000   75.000000  4.200000\n",
      "50%    149.990000  100.000000  4.300000\n",
      "75%    299.990000  150.000000  4.500000\n",
      "max    999.990000  200.000000  4.800000\n",
      "\n",
      "Summary of all columns:\n",
      "       Product     Category       Price       Stock    Rating\n",
      "count        5            5    5.000000    5.000000  5.000000\n",
      "unique       5            2         NaN         NaN       NaN\n",
      "top     Laptop  Accessories         NaN         NaN       NaN\n",
      "freq         1            3         NaN         NaN       NaN\n",
      "mean       NaN          NaN  311.990000  115.000000  4.360000\n",
      "std        NaN          NaN  397.831623   60.207973  0.304959\n",
      "min        NaN          NaN   29.990000   50.000000  4.000000\n",
      "25%        NaN          NaN   79.990000   75.000000  4.200000\n",
      "50%        NaN          NaN  149.990000  100.000000  4.300000\n",
      "75%        NaN          NaN  299.990000  150.000000  4.500000\n",
      "max        NaN          NaN  999.990000  200.000000  4.800000\n"
     ]
    }
   ],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical summary (numeric columns):\")\n",
    "print(df.describe())\n",
    "print()\n",
    "\n",
    "# Summary including non-numeric columns\n",
    "print(\"Summary of all columns:\")\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count:\n",
      "Product     0\n",
      "Category    0\n",
      "Price       0\n",
      "Stock       0\n",
      "Rating      0\n",
      "dtype: int64\n",
      "\n",
      "Unique values in 'Category':\n",
      "['Electronics' 'Accessories']\n",
      "Number of unique categories: 2\n",
      "\n",
      "Value counts for 'Category':\n",
      "Category\n",
      "Accessories    3\n",
      "Electronics    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values count:\")\n",
    "print(df.isnull().sum())\n",
    "print()\n",
    "\n",
    "# Unique values in categorical columns\n",
    "print(\"Unique values in 'Category':\")\n",
    "print(df['Category'].unique())\n",
    "print(f\"Number of unique categories: {df['Category'].nunique()}\")\n",
    "print()\n",
    "\n",
    "# Value counts\n",
    "print(\"Value counts for 'Category':\")\n",
    "print(df['Category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Selection and Indexing {#selection-indexing}\n",
    "\n",
    "Pandas provides multiple ways to select and filter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single column selection:\n",
      "0        Laptop\n",
      "1         Mouse\n",
      "2      Keyboard\n",
      "3       Monitor\n",
      "4    Headphones\n",
      "Name: Product, dtype: object\n",
      "Type: <class 'pandas.core.series.Series'>\n",
      "\n",
      "Multiple column selection:\n",
      "      Product   Price\n",
      "0      Laptop  999.99\n",
      "1       Mouse   29.99\n",
      "2    Keyboard   79.99\n",
      "3     Monitor  299.99\n",
      "4  Headphones  149.99\n",
      "Type: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Selecting columns\n",
    "print(\"Single column selection:\")\n",
    "print(df['Product'])\n",
    "print(f\"Type: {type(df['Product'])}\")\n",
    "print()\n",
    "\n",
    "# Multiple columns\n",
    "print(\"Multiple column selection:\")\n",
    "print(df[['Product', 'Price']])\n",
    "print(f\"Type: {type(df[['Product', 'Price']])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row (using iloc):\n",
      "Product          Laptop\n",
      "Category    Electronics\n",
      "Price            999.99\n",
      "Stock                50\n",
      "Rating              4.5\n",
      "Name: 0, dtype: object\n",
      "\n",
      "First 3 rows (using iloc):\n",
      "    Product     Category   Price  Stock  Rating\n",
      "0    Laptop  Electronics  999.99     50     4.5\n",
      "1     Mouse  Accessories   29.99    200     4.2\n",
      "2  Keyboard  Accessories   79.99    150     4.0\n",
      "\n",
      "Specific rows and columns (using iloc):\n",
      "      Category   Price\n",
      "0  Electronics  999.99\n",
      "1  Accessories   29.99\n",
      "2  Accessories   79.99\n"
     ]
    }
   ],
   "source": [
    "# Row selection by index\n",
    "print(\"First row (using iloc):\")\n",
    "print(df.iloc[0])\n",
    "print()\n",
    "\n",
    "# Multiple rows\n",
    "print(\"First 3 rows (using iloc):\")\n",
    "print(df.iloc[0:3])\n",
    "print()\n",
    "\n",
    "# Specific rows and columns\n",
    "print(\"Specific rows and columns (using iloc):\")\n",
    "print(df.iloc[0:3, 1:3])  # First 3 rows, columns 1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products with price > 100:\n",
      "      Product     Category   Price  Stock  Rating\n",
      "0      Laptop  Electronics  999.99     50     4.5\n",
      "3     Monitor  Electronics  299.99     75     4.8\n",
      "4  Headphones  Accessories  149.99    100     4.3\n",
      "\n",
      "Electronics with price > 200:\n",
      "   Product     Category   Price  Stock  Rating\n",
      "0   Laptop  Electronics  999.99     50     4.5\n",
      "3  Monitor  Electronics  299.99     75     4.8\n",
      "\n",
      "Products that are either Laptop or Monitor:\n",
      "   Product     Category   Price  Stock  Rating\n",
      "0   Laptop  Electronics  999.99     50     4.5\n",
      "3  Monitor  Electronics  299.99     75     4.8\n"
     ]
    }
   ],
   "source": [
    "# Boolean indexing (filtering)\n",
    "print(\"Products with price > 100:\")\n",
    "expensive_products = df[df['Price'] > 100]\n",
    "print(expensive_products)\n",
    "print()\n",
    "\n",
    "# Multiple conditions\n",
    "print(\"Electronics with price > 200:\")\n",
    "expensive_electronics = df[(df['Category'] == 'Electronics') & (df['Price'] > 200)]\n",
    "print(expensive_electronics)\n",
    "print()\n",
    "\n",
    "# Using isin() for multiple values\n",
    "print(\"Products that are either Laptop or Monitor:\")\n",
    "selected_products = df[df['Product'].isin(['Laptop', 'Monitor'])]\n",
    "print(selected_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Product as index:\n",
      "               Category   Price  Stock  Rating\n",
      "Product                                       \n",
      "Laptop      Electronics  999.99     50     4.5\n",
      "Mouse       Accessories   29.99    200     4.2\n",
      "Keyboard    Accessories   79.99    150     4.0\n",
      "Monitor     Electronics  299.99     75     4.8\n",
      "Headphones  Accessories  149.99    100     4.3\n",
      "\n",
      "Select Laptop row using loc:\n",
      "Category    Electronics\n",
      "Price            999.99\n",
      "Stock                50\n",
      "Rating              4.5\n",
      "Name: Laptop, dtype: object\n",
      "\n",
      "Select specific rows and columns using loc:\n",
      "           Price  Stock\n",
      "Product                \n",
      "Laptop    999.99     50\n",
      "Mouse      29.99    200\n",
      "Keyboard   79.99    150\n"
     ]
    }
   ],
   "source": [
    "# Using loc for label-based selection\n",
    "df_indexed = df.set_index('Product')\n",
    "print(\"DataFrame with Product as index:\")\n",
    "print(df_indexed)\n",
    "print()\n",
    "\n",
    "print(\"Select Laptop row using loc:\")\n",
    "print(df_indexed.loc['Laptop'])\n",
    "print()\n",
    "\n",
    "print(\"Select specific rows and columns using loc:\")\n",
    "print(df_indexed.loc['Laptop':'Keyboard', 'Price':'Stock'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning {#data-cleaning}\n",
    "\n",
    "Real-world data often contains missing values, duplicates, and inconsistencies that need to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original messy data:\n",
      "      Name   Age      City   Salary              Email\n",
      "0    Alice  25.0  New York  70000.0    alice@email.com\n",
      "1      Bob  30.0        LA  80000.0      bob@email.com\n",
      "2  Charlie   NaN   Chicago  90000.0  charlie@email.com\n",
      "3    Alice  25.0  New York  70000.0    alice@email.com\n",
      "4      Eve  45.0      None  85000.0      eve@email.com\n",
      "5    Frank  35.0    Boston      NaN    frank@email.com\n",
      "\n",
      "Missing values:\n",
      "Name      0\n",
      "Age       1\n",
      "City      1\n",
      "Salary    1\n",
      "Email     0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 1\n"
     ]
    }
   ],
   "source": [
    "# Create data with missing values and duplicates for demonstration\n",
    "messy_data = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'Eve', 'Frank'],\n",
    "    'Age': [25, 30, np.nan, 25, 45, 35],\n",
    "    'City': ['New York', 'LA', 'Chicago', 'New York', None, 'Boston'],\n",
    "    'Salary': [70000, 80000, 90000, 70000, 85000, np.nan],\n",
    "    'Email': ['alice@email.com', 'bob@email.com', 'charlie@email.com', \n",
    "              'alice@email.com', 'eve@email.com', 'frank@email.com']\n",
    "})\n",
    "\n",
    "print(\"Original messy data:\")\n",
    "print(messy_data)\n",
    "print(f\"\\nMissing values:\\n{messy_data.isnull().sum()}\")\n",
    "print(f\"\\nDuplicate rows: {messy_data.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "print(\"Methods to handle missing values:\\n\")\n",
    "\n",
    "# Drop rows with any missing values\n",
    "print(\"1. Drop rows with any missing values:\")\n",
    "clean_dropna = messy_data.dropna()\n",
    "print(clean_dropna)\n",
    "print(f\"Shape: {clean_dropna.shape}\")\n",
    "print()\n",
    "\n",
    "# Drop rows only if all values are missing\n",
    "print(\"2. Drop rows only if all values are missing:\")\n",
    "clean_dropna_all = messy_data.dropna(how='all')\n",
    "print(clean_dropna_all)\n",
    "print()\n",
    "\n",
    "# Fill missing values\n",
    "print(\"3. Fill missing values:\")\n",
    "clean_filled = messy_data.copy()\n",
    "clean_filled['Age'].fillna(clean_filled['Age'].mean(), inplace=True)\n",
    "clean_filled['City'].fillna('Unknown', inplace=True)\n",
    "clean_filled['Salary'].fillna(clean_filled['Salary'].median(), inplace=True)\n",
    "print(clean_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling duplicates\n",
    "print(\"Handling duplicates:\\n\")\n",
    "\n",
    "# Identify duplicates\n",
    "print(\"Duplicate rows:\")\n",
    "print(messy_data[messy_data.duplicated()])\n",
    "print()\n",
    "\n",
    "# Remove duplicates (keep first occurrence)\n",
    "print(\"After removing duplicates:\")\n",
    "clean_no_duplicates = messy_data.drop_duplicates()\n",
    "print(clean_no_duplicates)\n",
    "print(f\"Shape before: {messy_data.shape}\")\n",
    "print(f\"Shape after: {clean_no_duplicates.shape}\")\n",
    "print()\n",
    "\n",
    "# Remove duplicates based on specific columns\n",
    "print(\"Remove duplicates based on 'Name' and 'Email':\")\n",
    "clean_subset = messy_data.drop_duplicates(subset=['Name', 'Email'])\n",
    "print(clean_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type conversion\n",
    "print(\"Data type conversion:\\n\")\n",
    "\n",
    "# Create sample data with wrong types\n",
    "df_types = pd.DataFrame({\n",
    "    'ID': ['001', '002', '003', '004'],\n",
    "    'Score': ['85.5', '92.0', '78.5', '96.0'],\n",
    "    'Grade': ['A', 'A+', 'B', 'A+'],\n",
    "    'Pass': ['True', 'True', 'False', 'True']\n",
    "})\n",
    "\n",
    "print(\"Original data types:\")\n",
    "print(df_types.dtypes)\n",
    "print()\n",
    "\n",
    "# Convert data types\n",
    "df_types['ID'] = pd.to_numeric(df_types['ID'])\n",
    "df_types['Score'] = pd.to_numeric(df_types['Score'])\n",
    "df_types['Pass'] = df_types['Pass'].astype('bool')\n",
    "\n",
    "print(\"After conversion:\")\n",
    "print(df_types.dtypes)\n",
    "print()\n",
    "print(df_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Transformation {#data-transformation}\n",
    "\n",
    "Transforming data to create new insights or prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new columns\n",
    "df = sample_data.copy()\n",
    "print(\"Original data:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Add new columns\n",
    "df['Total_Value'] = df['Price'] * df['Stock']  # Calculate total inventory value\n",
    "df['Price_Category'] = pd.cut(df['Price'], \n",
    "                             bins=[0, 50, 200, float('inf')], \n",
    "                             labels=['Cheap', 'Medium', 'Expensive'])\n",
    "df['High_Rating'] = df['Rating'] >= 4.5\n",
    "\n",
    "print(\"With new columns:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String operations\n",
    "df['Product_Upper'] = df['Product'].str.upper()\n",
    "df['Product_Length'] = df['Product'].str.len()\n",
    "df['Category_Code'] = df['Category'].str[:4].str.upper()  # First 4 characters, uppercase\n",
    "\n",
    "print(\"String operations:\")\n",
    "print(df[['Product', 'Product_Upper', 'Product_Length', 'Category', 'Category_Code']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply custom functions\n",
    "def categorize_stock(stock):\n",
    "    if stock < 75:\n",
    "        return 'Low'\n",
    "    elif stock < 150:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "df['Stock_Level'] = df['Stock'].apply(categorize_stock)\n",
    "\n",
    "print(\"Custom function application:\")\n",
    "print(df[['Product', 'Stock', 'Stock_Level']])\n",
    "print()\n",
    "\n",
    "# Using lambda functions\n",
    "df['Discounted_Price'] = df['Price'].apply(lambda x: x * 0.9 if x > 100 else x)\n",
    "\n",
    "print(\"Lambda function (10% discount for items > $100):\")\n",
    "print(df[['Product', 'Price', 'Discounted_Price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting data\n",
    "print(\"Sorting by Price (ascending):\")\n",
    "print(df.sort_values('Price')[['Product', 'Price']])\n",
    "print()\n",
    "\n",
    "print(\"Sorting by multiple columns:\")\n",
    "print(df.sort_values(['Category', 'Price'], ascending=[True, False])[['Product', 'Category', 'Price']])\n",
    "print()\n",
    "\n",
    "# Ranking\n",
    "df['Price_Rank'] = df['Price'].rank(ascending=False)\n",
    "df['Rating_Rank'] = df['Rating'].rank(ascending=False)\n",
    "\n",
    "print(\"Rankings:\")\n",
    "print(df[['Product', 'Price', 'Price_Rank', 'Rating', 'Rating_Rank']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Grouping and Aggregation {#grouping-aggregation}\n",
    "\n",
    "Grouping data and performing aggregate operations is essential for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more comprehensive sample data\n",
    "sales_data = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=20, freq='D'),\n",
    "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones'] * 4,\n",
    "    'Category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Accessories'] * 4,\n",
    "    'Sales': np.random.randint(1, 10, 20),\n",
    "    'Revenue': np.random.randint(100, 1000, 20),\n",
    "    'Region': ['North', 'South', 'East', 'West', 'North'] * 4\n",
    "})\n",
    "\n",
    "print(\"Sales data:\")\n",
    "print(sales_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic grouping and aggregation\n",
    "print(\"Sales by Category:\")\n",
    "category_sales = sales_data.groupby('Category')['Sales'].sum()\n",
    "print(category_sales)\n",
    "print()\n",
    "\n",
    "print(\"Multiple aggregations:\")\n",
    "category_stats = sales_data.groupby('Category').agg({\n",
    "    'Sales': ['sum', 'mean', 'count'],\n",
    "    'Revenue': ['sum', 'mean', 'max', 'min']\n",
    "})\n",
    "print(category_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple grouping columns\n",
    "print(\"Sales by Category and Region:\")\n",
    "category_region_sales = sales_data.groupby(['Category', 'Region'])['Sales'].sum()\n",
    "print(category_region_sales)\n",
    "print()\n",
    "\n",
    "# Unstack for better visualization\n",
    "print(\"Unstacked format:\")\n",
    "print(category_region_sales.unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom aggregation functions\n",
    "def revenue_per_sale(group):\n",
    "    return group['Revenue'].sum() / group['Sales'].sum()\n",
    "\n",
    "print(\"Custom aggregation - Average Revenue per Sale:\")\n",
    "avg_revenue_per_sale = sales_data.groupby('Product').apply(revenue_per_sale)\n",
    "print(avg_revenue_per_sale)\n",
    "print()\n",
    "\n",
    "# Using transform to add group statistics back to original DataFrame\n",
    "sales_data['Category_Avg_Sales'] = sales_data.groupby('Category')['Sales'].transform('mean')\n",
    "sales_data['Sales_vs_Category_Avg'] = sales_data['Sales'] - sales_data['Category_Avg_Sales']\n",
    "\n",
    "print(\"Transform example (first 10 rows):\")\n",
    "print(sales_data[['Product', 'Category', 'Sales', 'Category_Avg_Sales', 'Sales_vs_Category_Avg']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Merging and Joining {#merging-joining}\n",
    "\n",
    "Combining data from multiple DataFrames is a common task in data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\n",
    "})\n",
    "\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104, 105, 106],\n",
    "    'customer_id': [1, 2, 2, 3, 1, 6],  # Note: customer_id 6 doesn't exist in customers\n",
    "    'product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones', 'Tablet'],\n",
    "    'amount': [999, 29, 79, 299, 149, 399]\n",
    "})\n",
    "\n",
    "print(\"Customers:\")\n",
    "print(customers)\n",
    "print(\"\\nOrders:\")\n",
    "print(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join (default) - only matching records\n",
    "print(\"Inner join:\")\n",
    "inner_join = pd.merge(customers, orders, on='customer_id')\n",
    "print(inner_join)\n",
    "print()\n",
    "\n",
    "# Left join - all records from left DataFrame\n",
    "print(\"Left join:\")\n",
    "left_join = pd.merge(customers, orders, on='customer_id', how='left')\n",
    "print(left_join)\n",
    "print()\n",
    "\n",
    "# Right join - all records from right DataFrame\n",
    "print(\"Right join:\")\n",
    "right_join = pd.merge(customers, orders, on='customer_id', how='right')\n",
    "print(right_join)\n",
    "print()\n",
    "\n",
    "# Outer join - all records from both DataFrames\n",
    "print(\"Outer join:\")\n",
    "outer_join = pd.merge(customers, orders, on='customer_id', how='outer')\n",
    "print(outer_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining with different column names\n",
    "products = pd.DataFrame({\n",
    "    'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones'],\n",
    "    'category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Accessories'],\n",
    "    'price': [999, 29, 79, 299, 149]\n",
    "})\n",
    "\n",
    "print(\"Products:\")\n",
    "print(products)\n",
    "print()\n",
    "\n",
    "# Merge with different column names\n",
    "order_details = pd.merge(orders, products, \n",
    "                        left_on='product', right_on='product_name', \n",
    "                        how='left')\n",
    "print(\"Order details (merged on different column names):\")\n",
    "print(order_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating DataFrames\n",
    "df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "df2 = pd.DataFrame({'A': [7, 8, 9], 'B': [10, 11, 12]})\n",
    "df3 = pd.DataFrame({'C': [13, 14, 15], 'D': [16, 17, 18]})\n",
    "\n",
    "print(\"Original DataFrames:\")\n",
    "print(\"df1:\")\n",
    "print(df1)\n",
    "print(\"\\ndf2:\")\n",
    "print(df2)\n",
    "print(\"\\ndf3:\")\n",
    "print(df3)\n",
    "print()\n",
    "\n",
    "# Vertical concatenation (stacking rows)\n",
    "print(\"Vertical concatenation (df1 + df2):\")\n",
    "vertical_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "print(vertical_concat)\n",
    "print()\n",
    "\n",
    "# Horizontal concatenation (side by side)\n",
    "print(\"Horizontal concatenation (df1 + df3):\")\n",
    "horizontal_concat = pd.concat([df1, df3], axis=1)\n",
    "print(horizontal_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Time Series Analysis {#time-series}\n",
    "\n",
    "Pandas has excellent support for working with dates and time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series data\n",
    "dates = pd.date_range('2024-01-01', periods=365, freq='D')\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'sales': np.random.normal(100, 20, 365) + np.sin(np.arange(365) * 2 * np.pi / 365) * 10,\n",
    "    'temperature': np.random.normal(20, 10, 365) + np.sin(np.arange(365) * 2 * np.pi / 365) * 15\n",
    "})\n",
    "\n",
    "# Set date as index\n",
    "ts_data.set_index('date', inplace=True)\n",
    "\n",
    "print(\"Time series data (first 10 rows):\")\n",
    "print(ts_data.head(10))\n",
    "print(f\"\\nData types:\\n{ts_data.dtypes}\")\n",
    "print(f\"\\nIndex type: {type(ts_data.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date/time operations\n",
    "ts_data['year'] = ts_data.index.year\n",
    "ts_data['month'] = ts_data.index.month\n",
    "ts_data['day_of_week'] = ts_data.index.day_name()\n",
    "ts_data['quarter'] = ts_data.index.quarter\n",
    "\n",
    "print(\"Date components:\")\n",
    "print(ts_data[['sales', 'year', 'month', 'day_of_week', 'quarter']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based selection\n",
    "print(\"January 2024 data:\")\n",
    "january_data = ts_data['2024-01']\n",
    "print(january_data.head())\n",
    "print(f\"Shape: {january_data.shape}\")\n",
    "print()\n",
    "\n",
    "# Date range selection\n",
    "print(\"First week of 2024:\")\n",
    "first_week = ts_data['2024-01-01':'2024-01-07']\n",
    "print(first_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling (aggregating by time periods)\n",
    "print(\"Monthly averages:\")\n",
    "monthly_avg = ts_data.resample('M')[['sales', 'temperature']].mean()\n",
    "print(monthly_avg)\n",
    "print()\n",
    "\n",
    "print(\"Weekly sums:\")\n",
    "weekly_sales = ts_data['sales'].resample('W').sum()\n",
    "print(weekly_sales.head(10))\n",
    "print()\n",
    "\n",
    "# Rolling calculations\n",
    "ts_data['sales_7day_avg'] = ts_data['sales'].rolling(window=7).mean()\n",
    "ts_data['sales_30day_avg'] = ts_data['sales'].rolling(window=30).mean()\n",
    "\n",
    "print(\"Rolling averages (first 40 rows):\")\n",
    "print(ts_data[['sales', 'sales_7day_avg', 'sales_30day_avg']].head(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Visualization with Pandas {#visualization}\n",
    "\n",
    "Pandas integrates well with matplotlib for quick data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Create sample data for visualization\n",
    "viz_data = pd.DataFrame({\n",
    "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones', 'Tablet'],\n",
    "    'Sales': [150, 300, 200, 120, 180, 90],\n",
    "    'Revenue': [149850, 8970, 15980, 35880, 26820, 35910],\n",
    "    'Rating': [4.5, 4.2, 4.0, 4.8, 4.3, 4.1]\n",
    "})\n",
    "\n",
    "print(\"Data for visualization:\")\n",
    "print(viz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "viz_data.set_index('Product')['Sales'].plot(kind='bar', color='skyblue')\n",
    "plt.title('Sales by Product')\n",
    "plt.xlabel('Product')\n",
    "plt.ylabel('Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Horizontal bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "viz_data.set_index('Product')['Revenue'].plot(kind='barh', color='lightcoral')\n",
    "plt.title('Revenue by Product')\n",
    "plt.xlabel('Revenue ($)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "viz_data.plot(x='Sales', y='Revenue', kind='scatter', s=100, alpha=0.7)\n",
    "plt.title('Sales vs Revenue')\n",
    "plt.xlabel('Sales')\n",
    "plt.ylabel('Revenue ($)')\n",
    "\n",
    "# Add product labels\n",
    "for i, row in viz_data.iterrows():\n",
    "    plt.annotate(row['Product'], (row['Sales'], row['Revenue']), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sales bar chart\n",
    "viz_data.set_index('Product')['Sales'].plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('Sales by Product')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Revenue pie chart\n",
    "viz_data.set_index('Product')['Revenue'].plot(kind='pie', ax=axes[0,1], autopct='%1.1f%%')\n",
    "axes[0,1].set_title('Revenue Distribution')\n",
    "axes[0,1].set_ylabel('')\n",
    "\n",
    "# Rating histogram\n",
    "viz_data['Rating'].plot(kind='hist', bins=10, ax=axes[1,0], color='lightgreen', alpha=0.7)\n",
    "axes[1,0].set_title('Rating Distribution')\n",
    "axes[1,0].set_xlabel('Rating')\n",
    "\n",
    "# Box plot\n",
    "viz_data[['Sales', 'Rating']].plot(kind='box', ax=axes[1,1])\n",
    "axes[1,1].set_title('Box Plot: Sales and Rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot original sales and moving averages\n",
    "ts_data['sales'].plot(label='Daily Sales', alpha=0.3)\n",
    "ts_data['sales_7day_avg'].plot(label='7-Day Moving Average', linewidth=2)\n",
    "ts_data['sales_30day_avg'].plot(label='30-Day Moving Average', linewidth=2)\n",
    "\n",
    "plt.title('Sales Time Series with Moving Averages')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Data Structures**: Master Series and DataFrame - they're the foundation of pandas\n",
    "2. **Data Loading**: Use appropriate `read_*` functions and specify parameters for optimal loading\n",
    "3. **Data Exploration**: Always start with `.info()`, `.describe()`, `.head()`, and `.tail()`\n",
    "4. **Data Cleaning**: Handle missing values and duplicates systematically\n",
    "5. **Selection**: Use `.loc[]` for label-based and `.iloc[]` for position-based selection\n",
    "6. **Grouping**: Leverage `groupby()` for powerful aggregations and analysis\n",
    "7. **Merging**: Choose the right join type based on your analysis needs\n",
    "8. **Time Series**: Set datetime as index for time-based operations\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Memory Management**: Use appropriate data types (`category` for categorical data, `int32` instead of `int64` when possible)\n",
    "- **Method Chaining**: Chain operations for cleaner code: `df.dropna().groupby('column').mean()`\n",
    "- **Copy vs. View**: Be aware of when you're creating copies vs. views of data\n",
    "- **Performance**: Use vectorized operations instead of loops when possible\n",
    "- **Documentation**: Document your data transformations and assumptions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore advanced pandas features like MultiIndex\n",
    "- Learn about pandas extensions and integrations with other libraries\n",
    "- Practice with real datasets to solidify your understanding\n",
    "- Consider learning complementary libraries like NumPy, Matplotlib, and Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up created files\n",
    "import os\n",
    "if os.path.exists('sample_products.csv'):\n",
    "    os.remove('sample_products.csv')\n",
    "    print(\"Cleaned up sample CSV file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
