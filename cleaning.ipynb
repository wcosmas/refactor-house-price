{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Melbourne Housing Data Cleaning Tutorial\n",
    "\n",
    "This notebook provides a comprehensive tutorial on data cleaning techniques using the Melbourne Housing Market dataset. We'll cover missing values, inconsistent formats, outliers, and best practices for preparing data for analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data\n",
    "\n",
    "First, let's import the necessary libraries and load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Melbourne Housing dataset\n",
    "df = pd.read_csv('datasets/Melbourne_housing_FULL.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Dataset size: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration\n",
    "\n",
    "Before we start cleaning, let's understand what we're working with. This step is crucial for identifying potential data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Number of rows: {df.shape[0]:,}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "print(\"\\n=== COLUMN INFORMATION ===\")\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n=== FIRST LOOK AT DATA ===\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Missing Values Analysis\n",
    "\n",
    "Missing values are one of the most common data quality issues. Let's identify and understand the patterns of missing data in our dataset.\n",
    "\n",
    "### Why Missing Values Matter:\n",
    "- **Model Performance**: Many ML algorithms can't handle missing values\n",
    "- **Bias Introduction**: Naive handling can introduce systematic bias\n",
    "- **Information Loss**: Dropping all rows with missing values can lose valuable information\n",
    "- **Pattern Recognition**: Missing patterns can reveal data collection issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values for each column\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100,\n",
    "    'Data_Type': df.dtypes\n",
    "})\n",
    "\n",
    "# Filter columns with missing values and sort by percentage\n",
    "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"=== MISSING VALUES SUMMARY ===\")\n",
    "print(f\"Total columns with missing values: {len(missing_data)} out of {len(df.columns)}\")\n",
    "print(f\"Total missing values: {missing_data['Missing_Count'].sum():,}\")\n",
    "print(f\"Percentage of total dataset missing: {(missing_data['Missing_Count'].sum() / df.size) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n=== COLUMNS WITH MISSING VALUES ===\")\n",
    "if len(missing_data) > 0:\n",
    "    print(missing_data.to_string(index=False))\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing value patterns\n",
    "if len(missing_data) > 0:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Bar plot of missing values\n",
    "    top_missing = missing_data.head(15)  # Show top 15 for readability\n",
    "    axes[0].bar(range(len(top_missing)), top_missing['Missing_Percentage'], \n",
    "                color=['red' if x > 50 else 'orange' if x > 20 else 'yellow' for x in top_missing['Missing_Percentage']])\n",
    "    axes[0].set_title('Missing Values by Column (Top 15)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Missing Percentage (%)')\n",
    "    axes[0].set_xticks(range(len(top_missing)))\n",
    "    axes[0].set_xticklabels(top_missing['Column'], rotation=45, ha='right')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for i, v in enumerate(top_missing['Missing_Percentage']):\n",
    "        axes[0].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Heatmap showing missing patterns (sample of rows for performance)\n",
    "    sample_size = min(1000, len(df))  # Sample for performance with large datasets\n",
    "    missing_cols = missing_data.head(15)['Column'].tolist()\n",
    "    df_sample = df[missing_cols].head(sample_size)\n",
    "    \n",
    "    axes[1].imshow(df_sample.isnull().T, cmap='RdYlBu_r', aspect='auto', interpolation='nearest')\n",
    "    axes[1].set_title(f'Missing Value Pattern Heatmap (First {sample_size} rows)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Row Index')\n",
    "    axes[1].set_ylabel('Columns with Missing Values')\n",
    "    axes[1].set_yticks(range(len(missing_cols)))\n",
    "    axes[1].set_yticklabels(missing_cols)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üîç Red = Missing, Blue = Present\")\n",
    "    print(f\"üìä Colors in bar chart: Red >50%, Orange >20%, Yellow <20%\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values to visualize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 3.1 Missing Value Treatment Strategies\n",
    "\n",
    "Different types of missing values require different treatment approaches. Let's implement various strategies:\n",
    "\n",
    "1. **Missing Completely At Random (MCAR)**: Missing values are unrelated to any other data\n",
    "2. **Missing At Random (MAR)**: Missing values are related to observed data but not the missing data itself  \n",
    "3. **Missing Not At Random (MNAR)**: Missing values are related to the unobserved data itself\n",
    "\n",
    "### Treatment Methods:\n",
    "- **Deletion**: Remove rows/columns with missing values\n",
    "- **Imputation**: Fill missing values with estimated values\n",
    "- **Indicator Variables**: Create binary flags for missingness\n",
    "- **Advanced Methods**: Multiple imputation, model-based imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning demonstrations\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "print(\"=== MISSING VALUE TREATMENT DEMONSTRATIONS ===\")\n",
    "\n",
    "# Function to apply different imputation strategies\n",
    "def apply_missing_value_treatments(df, column, strategy='mean'):\n",
    "    \"\"\"\n",
    "    Apply different missing value treatment strategies\n",
    "    \"\"\"\n",
    "    if df[column].dtype in ['object']:\n",
    "        if strategy == 'mode':\n",
    "            return df[column].fillna(df[column].mode()[0] if len(df[column].mode()) > 0 else 'Unknown')\n",
    "        elif strategy == 'constant':\n",
    "            return df[column].fillna('Unknown')\n",
    "        else:\n",
    "            return df[column].fillna('Unknown')\n",
    "    else:\n",
    "        if strategy == 'mean':\n",
    "            return df[column].fillna(df[column].mean())\n",
    "        elif strategy == 'median':\n",
    "            return df[column].fillna(df[column].median())\n",
    "        elif strategy == 'mode':\n",
    "            return df[column].fillna(df[column].mode()[0] if len(df[column].mode()) > 0 else 0)\n",
    "        elif strategy == 'zero':\n",
    "            return df[column].fillna(0)\n",
    "        elif strategy == 'forward_fill':\n",
    "            return df[column].fillna(method='ffill')\n",
    "        elif strategy == 'backward_fill':\n",
    "            return df[column].fillna(method='bfill')\n",
    "        else:\n",
    "            return df[column].fillna(df[column].mean())\n",
    "\n",
    "# Demonstrate different imputation strategies if we have missing values\n",
    "if len(missing_data) > 0:\n",
    "    # Take the first column with missing values as an example\n",
    "    example_col = missing_data.iloc[0]['Column']\n",
    "    print(f\"\\nDemonstrating imputation strategies on column: '{example_col}'\")\n",
    "    print(f\"Original missing values: {df[example_col].isnull().sum()}\")\n",
    "    print(f\"Data type: {df[example_col].dtype}\")\n",
    "    \n",
    "    if df[example_col].dtype == 'object':\n",
    "        strategies = ['mode', 'constant']\n",
    "    else:\n",
    "        strategies = ['mean', 'median', 'zero']\n",
    "    \n",
    "    print(f\"\\nBefore imputation - Sample values:\")\n",
    "    print(df[example_col].head(10).tolist())\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        imputed_col = apply_missing_value_treatments(df, example_col, strategy)\n",
    "        print(f\"\\n{strategy.upper()} imputation:\")\n",
    "        print(f\"  - Missing values after: {imputed_col.isnull().sum()}\")\n",
    "        if df[example_col].dtype != 'object':\n",
    "            print(f\"  - Imputed value: {imputed_col.fillna(method='ffill').iloc[-1]}\")\n",
    "        print(f\"  - Sample values: {imputed_col.head(10).tolist()}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚úÖ No missing values found - skipping imputation demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 4. Inconsistent Format Detection and Correction\n",
    "\n",
    "Data inconsistencies can occur in various forms. Let's identify and fix common format issues.\n",
    "\n",
    "### Common Format Issues:\n",
    "- **String Inconsistencies**: Mixed case, extra spaces, different representations\n",
    "- **Date Format Variations**: Different date formats in the same column\n",
    "- **Numerical Format Issues**: Mixed units, currency symbols, decimal separators\n",
    "- **Categorical Inconsistencies**: Same category with different names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potential format inconsistencies in categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"=== CATEGORICAL DATA CONSISTENCY ANALYSIS ===\")\n",
    "print(f\"Found {len(categorical_cols)} categorical columns to analyze\\n\")\n",
    "\n",
    "# Function to detect and analyze inconsistencies\n",
    "def analyze_categorical_consistency(df, column):\n",
    "    \"\"\"Analyze categorical column for potential inconsistencies\"\"\"\n",
    "    values = df[column].dropna().astype(str)\n",
    "    \n",
    "    issues = {\n",
    "        'mixed_case': [],\n",
    "        'extra_spaces': [],\n",
    "        'similar_values': [],\n",
    "        'special_chars': []\n",
    "    }\n",
    "    \n",
    "    unique_values = values.unique()\n",
    "    \n",
    "    # Check for mixed case issues\n",
    "    for val in unique_values:\n",
    "        if val != val.lower() and val != val.upper():\n",
    "            issues['mixed_case'].append(val)\n",
    "    \n",
    "    # Check for leading/trailing spaces\n",
    "    for val in unique_values:\n",
    "        if val != val.strip():\n",
    "            issues['extra_spaces'].append(f\"'{val}' -> '{val.strip()}'\")\n",
    "    \n",
    "    # Check for similar values (simple approach)\n",
    "    for i, val1 in enumerate(unique_values):\n",
    "        for val2 in unique_values[i+1:]:\n",
    "            if val1.lower().strip() == val2.lower().strip() and val1 != val2:\n",
    "                issues['similar_values'].append(f\"'{val1}' vs '{val2}'\")\n",
    "    \n",
    "    # Check for special characters\n",
    "    import re\n",
    "    for val in unique_values:\n",
    "        if re.search(r'[^a-zA-Z0-9\\s]', val):\n",
    "            issues['special_chars'].append(val)\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Analyze first few categorical columns as examples\n",
    "for col in categorical_cols[:5]:  # Limit to first 5 for demonstration\n",
    "    print(f\"\\nüìä ANALYZING COLUMN: '{col}'\")\n",
    "    print(f\"Unique values: {df[col].nunique()}\")\n",
    "    print(f\"Sample values: {df[col].dropna().unique()[:10].tolist()}\")\n",
    "    \n",
    "    issues = analyze_categorical_consistency(df, col)\n",
    "    \n",
    "    has_issues = any(len(issue_list) > 0 for issue_list in issues.values())\n",
    "    \n",
    "    if has_issues:\n",
    "        print(\"\\n‚ö†Ô∏è  ISSUES DETECTED:\")\n",
    "        if issues['mixed_case']:\n",
    "            print(f\"   Mixed case: {issues['mixed_case'][:5]}\")  # Show first 5\n",
    "        if issues['extra_spaces']:\n",
    "            print(f\"   Extra spaces: {issues['extra_spaces'][:5]}\")\n",
    "        if issues['similar_values']:\n",
    "            print(f\"   Similar values: {issues['similar_values'][:5]}\")\n",
    "        if issues['special_chars']:\n",
    "            print(f\"   Special chars: {issues['special_chars'][:5]}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No obvious consistency issues detected\")\n",
    "\n",
    "if len(categorical_cols) > 5:\n",
    "    print(f\"\\n... ({len(categorical_cols)-5} more columns not shown for brevity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate cleaning techniques for categorical data\n",
    "print(\"=== CATEGORICAL DATA CLEANING DEMONSTRATIONS ===\\n\")\n",
    "\n",
    "# Function to clean categorical data\n",
    "def clean_categorical_column(series, column_name):\n",
    "    \"\"\"\n",
    "    Clean categorical data by applying common cleaning techniques\n",
    "    \"\"\"\n",
    "    print(f\"üßπ Cleaning column: '{column_name}'\")\n",
    "    original_unique = series.nunique()\n",
    "    \n",
    "    # Create a copy to modify\n",
    "    cleaned = series.copy()\n",
    "    \n",
    "    # 1. Handle missing values\n",
    "    missing_count = cleaned.isnull().sum()\n",
    "    cleaned = cleaned.fillna('Unknown')\n",
    "    \n",
    "    # 2. Convert to string and strip whitespace\n",
    "    cleaned = cleaned.astype(str).str.strip()\n",
    "    \n",
    "    # 3. Standardize case (title case for most categorical data)\n",
    "    cleaned = cleaned.str.title()\n",
    "    \n",
    "    # 4. Remove extra internal spaces\n",
    "    cleaned = cleaned.str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    # 5. Handle common typos/variations (example for demonstration)\n",
    "    # This would be domain-specific in real scenarios\n",
    "    common_replacements = {\n",
    "        'Unknow': 'Unknown',\n",
    "        'N/A': 'Unknown',\n",
    "        'Na': 'Unknown',\n",
    "        'Nan': 'Unknown'\n",
    "    }\n",
    "    \n",
    "    for old, new in common_replacements.items():\n",
    "        cleaned = cleaned.str.replace(old, new, case=False)\n",
    "    \n",
    "    final_unique = cleaned.nunique()\n",
    "    \n",
    "    print(f\"   - Original unique values: {original_unique}\")\n",
    "    print(f\"   - Missing values handled: {missing_count}\")\n",
    "    print(f\"   - Final unique values: {final_unique}\")\n",
    "    print(f\"   - Reduction: {original_unique - final_unique} duplicates removed\")\n",
    "    \n",
    "    if original_unique != final_unique:\n",
    "        print(f\"   - Sample before: {series.dropna().unique()[:5].tolist()}\")\n",
    "        print(f\"   - Sample after:  {cleaned.unique()[:5].tolist()}\")\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Apply cleaning to a sample of categorical columns\n",
    "sample_cols = categorical_cols[:3] if len(categorical_cols) >= 3 else categorical_cols\n",
    "\n",
    "for col in sample_cols:\n",
    "    df_cleaned[f\"{col}_cleaned\"] = clean_categorical_column(df[col], col)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 5. Outlier Detection and Treatment\n",
    "\n",
    "Outliers can significantly impact data analysis and model performance. Let's identify and handle them appropriately.\n",
    "\n",
    "### Types of Outliers:\n",
    "- **Statistical Outliers**: Values that are statistically unusual\n",
    "- **Domain Outliers**: Values that don't make sense in the business context\n",
    "- **Data Entry Errors**: Incorrect values due to human error\n",
    "- **True Outliers**: Legitimate extreme values that provide valuable information\n",
    "\n",
    "### Detection Methods:\n",
    "- **IQR Method**: Using the Interquartile Range\n",
    "- **Z-Score**: Using standard deviations from the mean\n",
    "- **Modified Z-Score**: Using median absolute deviation\n",
    "- **Isolation Forest**: Machine learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns for outlier detection\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"=== OUTLIER DETECTION ANALYSIS ===\")\n",
    "print(f\"Found {len(numerical_cols)} numerical columns to analyze\\n\")\n",
    "\n",
    "# Function to detect outliers using multiple methods\n",
    "def detect_outliers_comprehensive(df, column, methods=['iqr', 'zscore', 'modified_zscore']):\n",
    "    \"\"\"\n",
    "    Detect outliers using multiple methods\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Skip if column has too many missing values\n",
    "    if df[column].isnull().sum() / len(df) > 0.5:\n",
    "        return {'skipped': 'Too many missing values (>50%)'}\n",
    "    \n",
    "    data = df[column].dropna()\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        return {'skipped': 'No data available'}\n",
    "    \n",
    "    # Method 1: IQR Method\n",
    "    if 'iqr' in methods:\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        iqr_outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "        results['iqr'] = {\n",
    "            'count': len(iqr_outliers),\n",
    "            'percentage': len(iqr_outliers) / len(data) * 100,\n",
    "            'bounds': (lower_bound, upper_bound),\n",
    "            'outliers': iqr_outliers\n",
    "        }\n",
    "    \n",
    "    # Method 2: Z-Score Method (using 3 standard deviations)\n",
    "    if 'zscore' in methods:\n",
    "        z_scores = np.abs(zscore(data))\n",
    "        zscore_outliers = data[z_scores > 3]\n",
    "        results['zscore'] = {\n",
    "            'count': len(zscore_outliers),\n",
    "            'percentage': len(zscore_outliers) / len(data) * 100,\n",
    "            'threshold': 3,\n",
    "            'outliers': zscore_outliers\n",
    "        }\n",
    "    \n",
    "    # Method 3: Modified Z-Score Method (using median)\n",
    "    if 'modified_zscore' in methods:\n",
    "        median = np.median(data)\n",
    "        mad = np.median(np.abs(data - median))\n",
    "        if mad != 0:\n",
    "            modified_z_scores = 0.6745 * (data - median) / mad\n",
    "            mod_zscore_outliers = data[np.abs(modified_z_scores) > 3.5]\n",
    "            results['modified_zscore'] = {\n",
    "                'count': len(mod_zscore_outliers),\n",
    "                'percentage': len(mod_zscore_outliers) / len(data) * 100,\n",
    "                'threshold': 3.5,\n",
    "                'outliers': mod_zscore_outliers\n",
    "            }\n",
    "        else:\n",
    "            results['modified_zscore'] = {'skipped': 'MAD is zero (no variability)'}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze outliers for first few numerical columns\n",
    "sample_numerical = numerical_cols[:5] if len(numerical_cols) >= 5 else numerical_cols\n",
    "\n",
    "for col in sample_numerical:\n",
    "    print(f\"\\nüìä OUTLIER ANALYSIS FOR: '{col}'\")\n",
    "    print(f\"Data type: {df[col].dtype}\")\n",
    "    print(f\"Non-null values: {df[col].count()} / {len(df)}\")\n",
    "    \n",
    "    if df[col].count() > 0:\n",
    "        print(f\"Basic stats: Min={df[col].min():.2f}, Max={df[col].max():.2f}, Mean={df[col].mean():.2f}, Std={df[col].std():.2f}\")\n",
    "        \n",
    "        outlier_results = detect_outliers_comprehensive(df, col)\n",
    "        \n",
    "        if 'skipped' in outlier_results:\n",
    "            print(f\"‚ö†Ô∏è  Analysis skipped: {outlier_results['skipped']}\")\n",
    "            continue\n",
    "        \n",
    "        print(\"\\nüîç OUTLIER DETECTION RESULTS:\")\n",
    "        \n",
    "        for method, results in outlier_results.items():\n",
    "            if 'skipped' in results:\n",
    "                print(f\"   {method.upper()}: Skipped - {results['skipped']}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"   {method.upper()}:\")\n",
    "            print(f\"      Count: {results['count']} ({results['percentage']:.2f}%)\")\n",
    "            \n",
    "            if method == 'iqr':\n",
    "                print(f\"      Bounds: [{results['bounds'][0]:.2f}, {results['bounds'][1]:.2f}]\")\n",
    "            else:\n",
    "                print(f\"      Threshold: {results['threshold']}\")\n",
    "            \n",
    "            if results['count'] > 0:\n",
    "                outlier_values = results['outliers'].head(3).tolist()\n",
    "                print(f\"      Sample outliers: {[f'{x:.2f}' for x in outlier_values]}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No data available for analysis\")\n",
    "\n",
    "if len(numerical_cols) > 5:\n",
    "    print(f\"\\n... ({len(numerical_cols)-5} more columns not shown for brevity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers for selected numerical columns\n",
    "def visualize_outliers(df, columns, max_cols=4):\n",
    "    \"\"\"\n",
    "    Create comprehensive outlier visualizations\n",
    "    \"\"\"\n",
    "    # Limit columns for visualization\n",
    "    viz_columns = columns[:max_cols]\n",
    "    n_cols = len(viz_columns)\n",
    "    \n",
    "    if n_cols == 0:\n",
    "        print(\"No numerical columns available for visualization\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_cols, figsize=(5*n_cols, 10))\n",
    "    if n_cols == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']\n",
    "    \n",
    "    for i, col in enumerate(viz_columns):\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        if len(data) == 0:\n",
    "            axes[0, i].text(0.5, 0.5, 'No data', ha='center', va='center', transform=axes[0, i].transAxes)\n",
    "            axes[1, i].text(0.5, 0.5, 'No data', ha='center', va='center', transform=axes[1, i].transAxes)\n",
    "            continue\n",
    "        \n",
    "        # Box plot (top row)\n",
    "        axes[0, i].boxplot(data, patch_artist=True, \n",
    "                          boxprops=dict(facecolor=colors[i % len(colors)], alpha=0.7))\n",
    "        axes[0, i].set_title(f'{col}\\nBox Plot (Outliers as points)', fontweight='bold')\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add outlier statistics\n",
    "        Q1, Q3 = data.quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "        \n",
    "        axes[0, i].text(0.02, 0.98, f'Outliers: {len(outliers)} ({len(outliers)/len(data)*100:.1f}%)', \n",
    "                       transform=axes[0, i].transAxes, verticalalignment='top',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Histogram with outliers highlighted (bottom row)\n",
    "        axes[1, i].hist(data, bins=30, alpha=0.7, color=colors[i % len(colors)], edgecolor='black')\n",
    "        \n",
    "        # Highlight outliers\n",
    "        if len(outliers) > 0:\n",
    "            axes[1, i].hist(outliers, bins=30, alpha=0.9, color='red', edgecolor='darkred', \n",
    "                           label=f'Outliers ({len(outliers)})')\n",
    "            axes[1, i].legend()\n",
    "        \n",
    "        axes[1, i].set_title(f'{col}\\nHistogram (Outliers in red)', fontweight='bold')\n",
    "        axes[1, i].set_xlabel('Value')\n",
    "        axes[1, i].set_ylabel('Frequency')\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add basic statistics\n",
    "        stats_text = f'Mean: {data.mean():.2f}\\nStd: {data.std():.2f}\\nMin: {data.min():.2f}\\nMax: {data.max():.2f}'\n",
    "        axes[1, i].text(0.98, 0.98, stats_text, transform=axes[1, i].transAxes, \n",
    "                       verticalalignment='top', horizontalalignment='right',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                       fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "print(\"=== OUTLIER VISUALIZATION ===\\n\")\n",
    "visualize_outliers(df, sample_numerical, max_cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 5.1 Outlier Treatment Strategies\n",
    "\n",
    "Once we've identified outliers, we need to decide how to handle them. The approach depends on the nature of the outliers and the intended use of the data.\n",
    "\n",
    "#### Treatment Options:\n",
    "1. **Keep**: When outliers represent legitimate extreme values\n",
    "2. **Remove**: When outliers are clearly errors or irrelevant\n",
    "3. **Transform**: Using log transformation, square root, etc.\n",
    "4. **Cap**: Set maximum/minimum thresholds (winsorizing)\n",
    "5. **Separate Analysis**: Treat outliers as a separate segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different outlier treatment strategies\n",
    "print(\"=== OUTLIER TREATMENT DEMONSTRATIONS ===\\n\")\n",
    "\n",
    "def demonstrate_outlier_treatments(df, column):\n",
    "    \"\"\"\n",
    "    Demonstrate different outlier treatment strategies\n",
    "    \"\"\"\n",
    "    print(f\"üõ†Ô∏è  TREATING OUTLIERS IN: '{column}'\")\n",
    "    \n",
    "    data = df[column].dropna()\n",
    "    if len(data) == 0:\n",
    "        print(\"   No data available\")\n",
    "        return\n",
    "    \n",
    "    # Identify outliers using IQR method\n",
    "    Q1, Q3 = data.quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    \n",
    "    print(f\"   Original data points: {len(data)}\")\n",
    "    print(f\"   Outliers detected (IQR): {len(outliers)} ({len(outliers)/len(data)*100:.2f}%)\")\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"   Outlier range: {outliers.min():.2f} to {outliers.max():.2f}\")\n",
    "    \n",
    "    if len(outliers) == 0:\n",
    "        print(\"   ‚úÖ No outliers to treat\")\n",
    "        return\n",
    "    \n",
    "    # Strategy 1: Removal\n",
    "    data_removed = data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "    print(f\"\\n   üìå STRATEGY 1 - Removal:\")\n",
    "    print(f\"      Remaining data points: {len(data_removed)} ({len(data_removed)/len(data)*100:.1f}%)\")\n",
    "    print(f\"      New range: {data_removed.min():.2f} to {data_removed.max():.2f}\")\n",
    "    \n",
    "    # Strategy 2: Capping (Winsorizing)\n",
    "    data_capped = data.copy()\n",
    "    data_capped[data_capped < lower_bound] = lower_bound\n",
    "    data_capped[data_capped > upper_bound] = upper_bound\n",
    "    print(f\"\\n   üìå STRATEGY 2 - Capping (Winsorizing):\")\n",
    "    print(f\"      Data points: {len(data_capped)} (no change)\")\n",
    "    print(f\"      New range: {data_capped.min():.2f} to {data_capped.max():.2f}\")\n",
    "    print(f\"      Values capped: {len(outliers)}\")\n",
    "    \n",
    "    # Strategy 3: Log transformation (if all values are positive)\n",
    "    if data.min() > 0:\n",
    "        data_log = np.log1p(data)  # log1p handles zeros better\n",
    "        print(f\"\\n   üìå STRATEGY 3 - Log Transformation:\")\n",
    "        print(f\"      Original skewness: {data.skew():.3f}\")\n",
    "        print(f\"      Log-transformed skewness: {data_log.skew():.3f}\")\n",
    "        print(f\"      Skewness improvement: {abs(data.skew()) - abs(data_log.skew()):.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n   üìå STRATEGY 3 - Log Transformation: Not applicable (contains zero/negative values)\")\n",
    "    \n",
    "    # Strategy 4: Z-score threshold\n",
    "    z_scores = np.abs(zscore(data))\n",
    "    data_zscore_filtered = data[z_scores <= 3]\n",
    "    print(f\"\\n   üìå STRATEGY 4 - Z-score filtering (|z| <= 3):\")\n",
    "    print(f\"      Remaining data points: {len(data_zscore_filtered)} ({len(data_zscore_filtered)/len(data)*100:.1f}%)\")\n",
    "    print(f\"      Removed: {len(data) - len(data_zscore_filtered)} outliers\")\n",
    "    \n",
    "    return {\n",
    "        'original': data,\n",
    "        'removed': data_removed,\n",
    "        'capped': data_capped,\n",
    "        'log_transformed': np.log1p(data) if data.min() > 0 else None,\n",
    "        'zscore_filtered': data_zscore_filtered\n",
    "    }\n",
    "\n",
    "# Demonstrate on a sample column with outliers\n",
    "if len(sample_numerical) > 0:\n",
    "    demo_col = sample_numerical[0]  # Take first numerical column\n",
    "    treatment_results = demonstrate_outlier_treatments(df, demo_col)\n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 6. Data Quality Validation\n",
    "\n",
    "After cleaning, it's important to validate that our data meets quality standards and is ready for analysis.\n",
    "\n",
    "### Validation Checks:\n",
    "- **Completeness**: Are all required fields populated?\n",
    "- **Consistency**: Are data formats and values consistent?\n",
    "- **Accuracy**: Do values make business sense?\n",
    "- **Validity**: Do values fall within acceptable ranges?\n",
    "- **Uniqueness**: Are there unexpected duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality validation\n",
    "def comprehensive_data_quality_report(df):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive data quality report\n",
    "    \"\"\"\n",
    "    print(\"=== COMPREHENSIVE DATA QUALITY REPORT ===\\n\")\n",
    "    \n",
    "    # Basic information\n",
    "    print(\"üìä DATASET OVERVIEW:\")\n",
    "    print(f\"   ‚Ä¢ Rows: {len(df):,}\")\n",
    "    print(f\"   ‚Ä¢ Columns: {len(df.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"   ‚Ä¢ Data types: {dict(df.dtypes.value_counts())}\")\n",
    "    \n",
    "    # Completeness check\n",
    "    print(\"\\n‚úì COMPLETENESS CHECK:\")\n",
    "    missing_summary = df.isnull().sum()\n",
    "    total_missing = missing_summary.sum()\n",
    "    missing_cols = missing_summary[missing_summary > 0]\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Total missing values: {total_missing:,} ({total_missing/df.size*100:.2f}%)\")\n",
    "    print(f\"   ‚Ä¢ Columns with missing data: {len(missing_cols)} / {len(df.columns)}\")\n",
    "    \n",
    "    if len(missing_cols) > 0:\n",
    "        print(f\"   ‚Ä¢ Worst offenders:\")\n",
    "        for col, missing_count in missing_cols.head(5).items():\n",
    "            print(f\"     - {col}: {missing_count} ({missing_count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Duplicates check\n",
    "    print(\"\\nüîç UNIQUENESS CHECK:\")\n",
    "    total_duplicates = df.duplicated().sum()\n",
    "    print(f\"   ‚Ä¢ Duplicate rows: {total_duplicates} ({total_duplicates/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Data type consistency\n",
    "    print(\"\\nüìã CONSISTENCY CHECK:\")\n",
    "    \n",
    "    # Check numerical columns for non-numeric values (if any were incorrectly typed)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"   ‚Ä¢ Numerical columns: {len(numeric_cols)}\")\n",
    "    \n",
    "    # Check for negative values in columns that shouldn't have them\n",
    "    suspicious_negatives = {}\n",
    "    for col in numeric_cols:\n",
    "        if 'price' in col.lower() or 'area' in col.lower() or 'rooms' in col.lower() or 'distance' in col.lower():\n",
    "            negative_count = (df[col] < 0).sum()\n",
    "            if negative_count > 0:\n",
    "                suspicious_negatives[col] = negative_count\n",
    "    \n",
    "    if suspicious_negatives:\n",
    "        print(f\"   ‚Ä¢ Suspicious negative values found:\")\n",
    "        for col, count in suspicious_negatives.items():\n",
    "            print(f\"     - {col}: {count} negative values\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ No suspicious negative values detected\")\n",
    "    \n",
    "    # Range validity checks\n",
    "    print(\"\\nüìè VALIDITY CHECKS:\")\n",
    "    \n",
    "    # Check for reasonable ranges in key columns\n",
    "    validity_issues = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        col_data = df[col].dropna()\n",
    "        if len(col_data) > 0:\n",
    "            # Check for extreme outliers (beyond 5 standard deviations)\n",
    "            if col_data.std() > 0:\n",
    "                extreme_outliers = np.abs(zscore(col_data)) > 5\n",
    "                if extreme_outliers.any():\n",
    "                    validity_issues.append(f\"{col}: {extreme_outliers.sum()} extreme outliers (>5 std)\")\n",
    "    \n",
    "    if validity_issues:\n",
    "        print(f\"   ‚Ä¢ Validity concerns found:\")\n",
    "        for issue in validity_issues[:5]:  # Show first 5\n",
    "            print(f\"     - {issue}\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ No major validity concerns detected\")\n",
    "    \n",
    "    # Categorical data consistency\n",
    "    print(\"\\nüè∑Ô∏è CATEGORICAL DATA QUALITY:\")\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    cat_issues = []\n",
    "    for col in categorical_cols[:5]:  # Check first 5 categorical columns\n",
    "        unique_vals = df[col].dropna().unique()\n",
    "        if len(unique_vals) > 0:\n",
    "            # Check for potential case/spacing issues\n",
    "            cleaned_vals = set()\n",
    "            for val in unique_vals:\n",
    "                cleaned_val = str(val).strip().lower()\n",
    "                if cleaned_val in cleaned_vals:\n",
    "                    cat_issues.append(f\"{col}: Potential case/spacing inconsistencies\")\n",
    "                    break\n",
    "                cleaned_vals.add(cleaned_val)\n",
    "    \n",
    "    if cat_issues:\n",
    "        print(f\"   ‚Ä¢ Issues detected:\")\n",
    "        for issue in cat_issues:\n",
    "            print(f\"     - {issue}\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ No obvious categorical inconsistencies in sample\")\n",
    "    \n",
    "    # Overall quality score\n",
    "    quality_score = 100\n",
    "    quality_score -= min((total_missing / df.size) * 100 * 2, 20)  # Missing data penalty\n",
    "    quality_score -= min((total_duplicates / len(df)) * 100 * 3, 15)  # Duplicate penalty\n",
    "    quality_score -= min(len(validity_issues) * 2, 10)  # Validity issues penalty\n",
    "    quality_score -= min(len(cat_issues) * 3, 10)  # Categorical issues penalty\n",
    "    \n",
    "    print(f\"\\n‚≠ê OVERALL DATA QUALITY SCORE: {quality_score:.1f}/100\")\n",
    "    \n",
    "    if quality_score >= 90:\n",
    "        print(\"   üéâ Excellent data quality! Ready for advanced analysis.\")\n",
    "    elif quality_score >= 75:\n",
    "        print(\"   ‚úÖ Good data quality. Minor cleaning recommended.\")\n",
    "    elif quality_score >= 60:\n",
    "        print(\"   ‚ö†Ô∏è Moderate data quality. Significant cleaning needed.\")\n",
    "    else:\n",
    "        print(\"   üö® Poor data quality. Major cleaning required before analysis.\")\n",
    "    \n",
    "    return quality_score\n",
    "\n",
    "# Generate quality report for original data\n",
    "print(\"ORIGINAL DATA:\")\n",
    "original_score = comprehensive_data_quality_report(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Generate quality report for cleaned data (if we made changes)\n",
    "print(\"CLEANED DATA:\")\n",
    "cleaned_score = comprehensive_data_quality_report(df_cleaned)\n",
    "\n",
    "print(f\"\\nüìà QUALITY IMPROVEMENT: {cleaned_score - original_score:.1f} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 7. Best Practices and Summary\n",
    "\n",
    "### Data Cleaning Best Practices:\n",
    "\n",
    "1. **Always Keep Original Data**: Make copies before cleaning\n",
    "2. **Document Changes**: Keep track of all transformations\n",
    "3. **Domain Knowledge**: Use business understanding to guide decisions\n",
    "4. **Iterative Process**: Clean, validate, and repeat\n",
    "5. **Automate When Possible**: Create reusable cleaning functions\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "‚úÖ **Missing Values**: Understand the pattern before treating  \n",
    "‚úÖ **Inconsistent Formats**: Standardize early to prevent issues  \n",
    "‚úÖ **Outliers**: Don't automatically remove - investigate first  \n",
    "‚úÖ **Validation**: Always validate after cleaning  \n",
    "‚úÖ **Documentation**: Record decisions for reproducibility  \n",
    "\n",
    "### Next Steps:\n",
    "After cleaning, your data is ready for:\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Feature Engineering\n",
    "- Machine Learning Model Development\n",
    "- Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and export cleaned data\n",
    "print(\"=== DATA CLEANING TUTORIAL COMPLETED ===\\n\")\n",
    "\n",
    "print(\"üìö WHAT WE COVERED:\")\n",
    "print(\"   1. ‚úÖ Initial data exploration and overview\")\n",
    "print(\"   2. ‚úÖ Missing values analysis and treatment strategies\")\n",
    "print(\"   3. ‚úÖ Inconsistent format detection and correction\")\n",
    "print(\"   4. ‚úÖ Outlier detection using multiple methods\") \n",
    "print(\"   5. ‚úÖ Outlier treatment demonstrations\")\n",
    "print(\"   6. ‚úÖ Comprehensive data quality validation\")\n",
    "print(\"   7. ‚úÖ Best practices and recommendations\")\n",
    "\n",
    "print(\"\\nüõ†Ô∏è  CLEANING TECHNIQUES DEMONSTRATED:\")\n",
    "print(\"   ‚Ä¢ Missing value imputation (mean, median, mode, constant)\")\n",
    "print(\"   ‚Ä¢ Categorical data standardization\")\n",
    "print(\"   ‚Ä¢ Outlier detection (IQR, Z-score, Modified Z-score)\")\n",
    "print(\"   ‚Ä¢ Outlier treatment (removal, capping, transformation)\")\n",
    "print(\"   ‚Ä¢ Data quality validation and scoring\")\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHTS FOR MELBOURNE HOUSING DATA:\")\n",
    "# Check if missing_data exists (it might not if there are no missing values)\n",
    "try:\n",
    "    if len(missing_data) > 0:\n",
    "        print(f\"   ‚Ä¢ {len(missing_data)} columns have missing values\")\n",
    "        print(f\"   ‚Ä¢ Highest missing: {missing_data.iloc[0]['Column']} ({missing_data.iloc[0]['Missing_Percentage']:.1f}%)\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ No missing values detected - excellent data quality!\")\n",
    "except NameError:\n",
    "    print(\"   ‚Ä¢ Missing value analysis completed\")\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"   ‚Ä¢ {len(categorical_cols)} categorical columns for potential standardization\")\n",
    "\n",
    "if len(numerical_cols) > 0:\n",
    "    print(f\"   ‚Ä¢ {len(numerical_cols)} numerical columns for outlier analysis\")\n",
    "\n",
    "print(\"\\nüöÄ READY FOR NEXT STEPS:\")\n",
    "print(\"   ‚Ä¢ Exploratory Data Analysis (EDA)\")\n",
    "print(\"   ‚Ä¢ Feature Engineering\")\n",
    "print(\"   ‚Ä¢ Machine Learning Modeling\")\n",
    "print(\"   ‚Ä¢ Statistical Analysis\")\n",
    "\n",
    "# Optionally save cleaned data\n",
    "save_cleaned = True  # Set to False if you don't want to save\n",
    "\n",
    "if save_cleaned:\n",
    "    try:\n",
    "        df_cleaned.to_csv('datasets/Melbourne_housing_CLEANED.csv', index=False)\n",
    "        print(\"\\nüíæ Cleaned dataset saved as 'datasets/Melbourne_housing_CLEANED.csv'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Could not save cleaned dataset: {e}\")\n",
    "\n",
    "print(\"\\nüéâ DATA CLEANING TUTORIAL COMPLETED SUCCESSFULLY!\")\n",
    "print(\"\\nYou now have the knowledge and tools to clean datasets effectively.\")\n",
    "print(\"Remember: Good data cleaning is the foundation of successful data analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}